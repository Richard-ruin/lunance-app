# scripts/train/model_trainer.py - ADVANCED MODEL TRAINER
import os
import json
import logging
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, f1_score
from sklearn.utils.class_weight import compute_class_weight

logger = logging.getLogger(__name__)

# Check and import required packages
def check_training_dependencies():
    """Check if training dependencies are available"""
    required_packages = {
        'torch': 'torch',
        'transformers': 'transformers',
        'datasets': 'datasets',
        'sklearn': 'scikit-learn'
    }
    
    missing = []
    for package, pip_name in required_packages.items():
        try:
            __import__(package)
        except ImportError:
            missing.append(pip_name)
    
    if missing:
        logger.error(f"Missing packages: {missing}")
        logger.error(f"Install with: pip install {' '.join(missing)}")
        return False
    return True

if check_training_dependencies():
    import torch
    from transformers import (
        AutoTokenizer, AutoModelForSequenceClassification,
        TrainingArguments, Trainer, EarlyStoppingCallback,
        DataCollatorWithPadding
    )
    from datasets import Dataset
    logger.info(f"ðŸš€ Training dependencies loaded. PyTorch: {torch.__version__}")
else:
    logger.error("âŒ Training dependencies not available")
    exit(1)

class ModelTrainer:
    """
    Advanced model trainer with balanced approach and multiple classifiers
    """
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ðŸ”§ Using device: {self.device}")
        
        # Setup random seeds for reproducibility
        self._set_seeds(self.config.validation_seed)
    
    def _set_seeds(self, seed: int):
        """Set random seeds for reproducibility"""
        import random
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
        logger.info(f"ðŸŽ² Random seeds set to {seed}")
    
    def train_all_models(self) -> Dict[str, Any]:
        """Train all models with advanced configuration"""
        logger.info("ðŸš€ Starting advanced model training pipeline...")
        
        results = {}
        start_time = datetime.now()
        
        try:
            # Train Intent Classifier
            logger.info("\n" + "ðŸŽ¯" * 20)
            logger.info("TRAINING INTENT CLASSIFIER")
            logger.info("ðŸŽ¯" * 20)
            
            intent_results = self.train_intent_classifier()
            results["intent_classifier"] = intent_results
            
            # Train Category Classifier
            logger.info("\n" + "ðŸ·ï¸" * 20)
            logger.info("TRAINING CATEGORY CLASSIFIER")
            logger.info("ðŸ·ï¸" * 20)
            
            category_results = self.train_category_classifier()
            results["category_classifier"] = category_results
            
            # Train Query Classifier - NEW
            logger.info("\n" + "â“" * 20)
            logger.info("TRAINING QUERY CLASSIFIER")
            logger.info("â“" * 20)
            
            query_results = self.train_query_classifier()
            results["query_classifier"] = query_results
            
            # Generate training summary
            end_time = datetime.now()
            duration = end_time - start_time
            
            summary = self._generate_training_summary(results, start_time, end_time, duration)
            self._save_training_summary(summary)
            
            logger.info("\nðŸŽ‰ TRAINING PIPELINE COMPLETED SUCCESSFULLY!")
            logger.info(f"â±ï¸ Total duration: {duration}")
            logger.info(f"ðŸ“ Models saved to: {self.config.output_dir}")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ Training pipeline failed: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def train_intent_classifier(self) -> Dict[str, Any]:
        """Train intent classifier with balanced approach"""
        try:
            # Load dataset
            dataset_path = os.path.join(self.config.data_dir, "intent_dataset.json")
            with open(dataset_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            df = pd.DataFrame(data)
            logger.info(f"ðŸ“Š Intent dataset loaded: {len(df)} samples")
            
            # Analyze class distribution
            class_counts = df['label'].value_counts()
            logger.info("ðŸ“ˆ Intent class distribution:")
            for label, count in class_counts.items():
                logger.info(f"  {label}: {count} samples ({count/len(df)*100:.1f}%)")
            
            # Create label mappings
            unique_labels = sorted(df['label'].unique())
            label2id = {label: i for i, label in enumerate(unique_labels)}
            id2label = {i: label for label, i in label2id.items()}
            
            # Prepare data
            texts = df['text'].tolist()
            labels = [label2id[label] for label in df['label']]
            
            # Balanced train-test split
            train_texts, val_texts, train_labels, val_labels = train_test_split(
                texts, labels,
                test_size=self.config.test_size,
                random_state=self.config.validation_seed,
                stratify=labels
            )
            
            logger.info(f"ðŸ“Š Train: {len(train_texts)}, Validation: {len(val_texts)}")
            
            # Calculate class weights for balanced training
            class_weights = None
            if self.config.use_class_weights:
                class_weights = compute_class_weight(
                    'balanced',
                    classes=np.unique(train_labels),
                    y=train_labels
                )
                logger.info(f"âš–ï¸ Class weights: {dict(zip(unique_labels, class_weights))}")
            
            # Load model and tokenizer
            tokenizer = AutoTokenizer.from_pretrained(self.config.base_model)
            model = AutoModelForSequenceClassification.from_pretrained(
                self.config.base_model,
                num_labels=len(unique_labels),
                label2id=label2id,
                id2label=id2label,
                problem_type="single_label_classification"
            )
            
            # Tokenize datasets
            train_dataset = self._create_dataset(train_texts, train_labels, tokenizer)
            val_dataset = self._create_dataset(val_texts, val_labels, tokenizer)
            
            # Training arguments
            training_args = self._create_training_args(
                self.config.intent_model_dir,
                "intent_classifier"
            )
            
            # Data collator
            data_collator = DataCollatorWithPadding(
                tokenizer=tokenizer,
                padding=True,
                return_tensors="pt"
            )
            
            # Compute metrics function
            def compute_metrics(eval_pred):
                predictions, labels = eval_pred
                predictions = np.argmax(predictions, axis=1)
                
                accuracy = accuracy_score(labels, predictions)
                f1_macro = f1_score(labels, predictions, average='macro')
                f1_weighted = f1_score(labels, predictions, average='weighted')
                
                report = classification_report(labels, predictions, output_dict=True, zero_division=0)
                
                return {
                    "accuracy": accuracy,
                    "f1_macro": f1_macro,
                    "f1_weighted": f1_weighted,
                    "precision_macro": report['macro avg']['precision'],
                    "recall_macro": report['macro avg']['recall']
                }
            
            # Create trainer
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=val_dataset,
                tokenizer=tokenizer,
                data_collator=data_collator,
                compute_metrics=compute_metrics,
                callbacks=[EarlyStoppingCallback(early_stopping_patience=self.config.patience)]
            )
            
            # Train model
            logger.info("ðŸš€ Starting intent classifier training...")
            train_result = trainer.train()
            
            # Evaluate model
            eval_result = trainer.evaluate()
            logger.info("ðŸ“Š Intent classifier evaluation:")
            for key, value in eval_result.items():
                logger.info(f"  {key}: {value:.4f}")
            
            # Save model
            logger.info(f"ðŸ’¾ Saving intent classifier to {self.config.intent_model_dir}")
            trainer.save_model()
            tokenizer.save_pretrained(self.config.intent_model_dir)
            
            # Save metadata
            metadata = {
                "model_info": {
                    "model_type": "intent_classifier",
                    "base_model": self.config.base_model,
                    "num_labels": len(unique_labels),
                    "labels": unique_labels,
                    "training_samples": len(train_texts),
                    "validation_samples": len(val_texts)
                },
                "training_config": {
                    "epochs": self.config.num_train_epochs,
                    "batch_size": self.config.train_batch_size,
                    "learning_rate": self.config.learning_rate,
                    "warmup_steps": self.config.warmup_steps,
                    "use_class_weights": self.config.use_class_weights,
                    "early_stopping": self.config.use_early_stopping
                },
                "results": {
                    "train_loss": train_result.training_loss,
                    **eval_result
                },
                "label_mapping": {
                    "label2id": label2id,
                    "id2label": id2label
                },
                "class_distribution": class_counts.to_dict(),
                "trained_at": datetime.now().isoformat()
            }
            
            metadata_path = os.path.join(self.config.intent_model_dir, "model_metadata.json")
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            logger.info("âœ… Intent classifier training completed!")
            return eval_result
            
        except Exception as e:
            logger.error(f"âŒ Intent classifier training failed: {e}")
            raise
    
    def train_category_classifier(self) -> Dict[str, Any]:
        """Train category classifier"""
        try:
            # Load dataset
            dataset_path = os.path.join(self.config.data_dir, "category_dataset.json")
            with open(dataset_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            df = pd.DataFrame(data)
            logger.info(f"ðŸ“Š Category dataset loaded: {len(df)} samples")
            
            # Analyze class distribution
            class_counts = df['label'].value_counts()
            logger.info(f"ðŸ“ˆ Category classes: {len(class_counts)} categories")
            
            # Create label mappings
            unique_labels = sorted(df['label'].unique())
            label2id = {label: i for i, label in enumerate(unique_labels)}
            id2label = {i: label for label, i in label2id.items()}
            
            # Prepare data
            texts = df['text'].tolist()
            labels = [label2id[label] for label in df['label']]
            
            # Split data
            train_texts, val_texts, train_labels, val_labels = train_test_split(
                texts, labels,
                test_size=self.config.test_size,
                random_state=self.config.validation_seed,
                stratify=labels
            )
            
            logger.info(f"ðŸ“Š Train: {len(train_texts)}, Validation: {len(val_texts)}")
            
            # Load model and tokenizer
            tokenizer = AutoTokenizer.from_pretrained(self.config.base_model)
            model = AutoModelForSequenceClassification.from_pretrained(
                self.config.base_model,
                num_labels=len(unique_labels),
                label2id=label2id,
                id2label=id2label
            )
            
            # Create datasets
            train_dataset = self._create_dataset(train_texts, train_labels, tokenizer)
            val_dataset = self._create_dataset(val_texts, val_labels, tokenizer)
            
            # Training arguments
            training_args = self._create_training_args(
                self.config.category_model_dir,
                "category_classifier"
            )
            
            # Data collator
            data_collator = DataCollatorWithPadding(
                tokenizer=tokenizer,
                padding=True,
                return_tensors="pt"
            )
            
            # Compute metrics
            def compute_metrics(eval_pred):
                predictions, labels = eval_pred
                predictions = np.argmax(predictions, axis=1)
                
                accuracy = accuracy_score(labels, predictions)
                f1_macro = f1_score(labels, predictions, average='macro')
                f1_weighted = f1_score(labels, predictions, average='weighted')
                
                return {
                    "accuracy": accuracy,
                    "f1_macro": f1_macro,
                    "f1_weighted": f1_weighted
                }
            
            # Create trainer
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=val_dataset,
                tokenizer=tokenizer,
                data_collator=data_collator,
                compute_metrics=compute_metrics,
                callbacks=[EarlyStoppingCallback(early_stopping_patience=self.config.patience)]
            )
            
            # Train
            logger.info("ðŸš€ Starting category classifier training...")
            train_result = trainer.train()
            
            # Evaluate
            eval_result = trainer.evaluate()
            logger.info("ðŸ“Š Category classifier evaluation:")
            for key, value in eval_result.items():
                logger.info(f"  {key}: {value:.4f}")
            
            # Save model
            logger.info(f"ðŸ’¾ Saving category classifier to {self.config.category_model_dir}")
            trainer.save_model()
            tokenizer.save_pretrained(self.config.category_model_dir)
            
            # Save metadata
            metadata = {
                "model_info": {
                    "model_type": "category_classifier",
                    "base_model": self.config.base_model,
                    "num_labels": len(unique_labels),
                    "training_samples": len(train_texts),
                    "validation_samples": len(val_texts)
                },
                "results": {
                    "train_loss": train_result.training_loss,
                    **eval_result
                },
                "label_mapping": {
                    "label2id": label2id,
                    "id2label": id2label
                },
                "trained_at": datetime.now().isoformat()
            }
            
            metadata_path = os.path.join(self.config.category_model_dir, "model_metadata.json")
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            logger.info("âœ… Category classifier training completed!")
            return eval_result
            
        except Exception as e:
            logger.error(f"âŒ Category classifier training failed: {e}")
            raise
    
    def train_query_classifier(self) -> Dict[str, Any]:
        """Train query classifier - NEW FEATURE"""
        try:
            # Load dataset
            dataset_path = os.path.join(self.config.data_dir, "query_dataset.json")
            with open(dataset_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            df = pd.DataFrame(data)
            logger.info(f"ðŸ“Š Query dataset loaded: {len(df)} samples")
            
            # Analyze class distribution
            class_counts = df['label'].value_counts()
            logger.info("ðŸ“ˆ Query class distribution:")
            for label, count in class_counts.items():
                logger.info(f"  {label}: {count} samples")
            
            # Create label mappings
            unique_labels = sorted(df['label'].unique())
            label2id = {label: i for i, label in enumerate(unique_labels)}
            id2label = {i: label for label, i in label2id.items()}
            
            # Prepare data
            texts = df['text'].tolist()
            labels = [label2id[label] for label in df['label']]
            
            # Split data
            train_texts, val_texts, train_labels, val_labels = train_test_split(
                texts, labels,
                test_size=self.config.test_size,
                random_state=self.config.validation_seed,
                stratify=labels
            )
            
            logger.info(f"ðŸ“Š Train: {len(train_texts)}, Validation: {len(val_texts)}")
            
            # Load model and tokenizer
            tokenizer = AutoTokenizer.from_pretrained(self.config.base_model)
            model = AutoModelForSequenceClassification.from_pretrained(
                self.config.base_model,
                num_labels=len(unique_labels),
                label2id=label2id,
                id2label=id2label
            )
            
            # Create datasets
            train_dataset = self._create_dataset(train_texts, train_labels, tokenizer)
            val_dataset = self._create_dataset(val_texts, val_labels, tokenizer)
            
            # Training arguments
            training_args = self._create_training_args(
                self.config.query_model_dir,
                "query_classifier"
            )
            
            # Data collator
            data_collator = DataCollatorWithPadding(
                tokenizer=tokenizer,
                padding=True,
                return_tensors="pt"
            )
            
            # Compute metrics
            def compute_metrics(eval_pred):
                predictions, labels = eval_pred
                predictions = np.argmax(predictions, axis=1)
                
                accuracy = accuracy_score(labels, predictions)
                f1_macro = f1_score(labels, predictions, average='macro')
                
                return {
                    "accuracy": accuracy,
                    "f1_macro": f1_macro
                }
            
            # Create trainer
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=val_dataset,
                tokenizer=tokenizer,
                data_collator=data_collator,
                compute_metrics=compute_metrics,
                callbacks=[EarlyStoppingCallback(early_stopping_patience=self.config.patience)]
            )
            
            # Train
            logger.info("ðŸš€ Starting query classifier training...")
            train_result = trainer.train()
            
            # Evaluate
            eval_result = trainer.evaluate()
            logger.info("ðŸ“Š Query classifier evaluation:")
            for key, value in eval_result.items():
                logger.info(f"  {key}: {value:.4f}")
            
            # Save model
            logger.info(f"ðŸ’¾ Saving query classifier to {self.config.query_model_dir}")
            trainer.save_model()
            tokenizer.save_pretrained(self.config.query_model_dir)
            
            # Save metadata
            metadata = {
                "model_info": {
                    "model_type": "query_classifier",
                    "base_model": self.config.base_model,
                    "num_labels": len(unique_labels),
                    "training_samples": len(train_texts),
                    "validation_samples": len(val_texts)
                },
                "results": {
                    "train_loss": train_result.training_loss,
                    **eval_result
                },
                "label_mapping": {
                    "label2id": label2id,
                    "id2label": id2label
                },
                "trained_at": datetime.now().isoformat()
            }
            
            metadata_path = os.path.join(self.config.query_model_dir, "model_metadata.json")
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            logger.info("âœ… Query classifier training completed!")
            return eval_result
            
        except Exception as e:
            logger.error(f"âŒ Query classifier training failed: {e}")
            raise
    
    def _create_dataset(self, texts: List[str], labels: List[int], tokenizer) -> Dataset:
        """Create tokenized dataset"""
        def tokenize_function(examples):
            return tokenizer(
                examples["text"],
                truncation=True,
                padding=True,
                max_length=128,
                return_tensors="pt"
            )
        
        # Create dataset
        dataset = Dataset.from_dict({
            "text": texts,
            "labels": labels
        })
        
        # Tokenize
        dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=["text"]
        )
        
        return dataset
    
    def _create_training_args(self, output_dir: str, model_type: str) -> TrainingArguments:
        """Create training arguments"""
        return TrainingArguments(
            output_dir=output_dir,
            
            # Training parameters
            num_train_epochs=self.config.num_train_epochs,
            per_device_train_batch_size=self.config.train_batch_size,
            per_device_eval_batch_size=self.config.eval_batch_size,
            gradient_accumulation_steps=1,
            
            # Learning rate
            learning_rate=self.config.learning_rate,
            weight_decay=0.01,
            warmup_steps=self.config.warmup_steps,
            lr_scheduler_type="linear",
            
            # Evaluation and saving
            eval_strategy="steps",
            eval_steps=200,
            save_strategy="steps",
            save_steps=200,
            save_total_limit=2,
            
            # Early stopping
            load_best_model_at_end=True,
            metric_for_best_model="eval_accuracy",
            greater_is_better=True,
            
            # Performance
            dataloader_num_workers=0,
            remove_unused_columns=True,
            fp16=False,  # Disable for stability
            
            # Logging
            logging_dir=f"{output_dir}/logs",
            logging_steps=50,
            report_to="none",
            
            # Optimization
            optim="adamw_torch",
            max_grad_norm=1.0,
            
            # Seeding
            seed=self.config.validation_seed,
            data_seed=self.config.validation_seed,
            
            # Additional stability
            dataloader_pin_memory=False,
            skip_memory_metrics=True,
        )
    
    def _generate_training_summary(self, results: Dict[str, Any], start_time: datetime, 
                                 end_time: datetime, duration) -> Dict[str, Any]:
        """Generate comprehensive training summary"""
        return {
            "training_info": {
                "started_at": start_time.isoformat(),
                "completed_at": end_time.isoformat(),
                "total_duration": str(duration),
                "duration_minutes": duration.total_seconds() / 60,
                "config_preset": "balanced",
                "base_model": self.config.base_model
            },
            "model_results": results,
            "model_paths": {
                "intent_classifier": self.config.intent_model_dir,
                "category_classifier": self.config.category_model_dir,
                "query_classifier": self.config.query_model_dir
            },
            "training_features": [
                "Balanced dataset generation",
                "Reduced financial detection bias",
                "Enhanced non-financial examples",
                "Query detection capability",
                "Class weight balancing",
                "Early stopping mechanism",
                "Stratified train-test split",
                "Advanced model configuration"
            ],
            "improvements_over_previous": [
                "Better balance between financial and non-financial",
                "Separate query detection model",
                "Reduced overfitting to financial patterns",
                "Enhanced conversation understanding",
                "More robust negative examples",
                "Improved category distribution"
            ],
            "performance_summary": {
                "intent_accuracy": results.get("intent_classifier", {}).get("eval_accuracy", 0),
                "category_accuracy": results.get("category_classifier", {}).get("eval_accuracy", 0),
                "query_accuracy": results.get("query_classifier", {}).get("eval_accuracy", 0),
                "average_accuracy": sum([
                    results.get("intent_classifier", {}).get("eval_accuracy", 0),
                    results.get("category_classifier", {}).get("eval_accuracy", 0),
                    results.get("query_classifier", {}).get("eval_accuracy", 0)
                ]) / 3
            },
            "next_steps": [
                "Test models with validation dataset",
                "Deploy to production environment", 
                "Monitor performance on real conversations",
                "Collect feedback for future improvements"
            ]
        }
    
    def _save_training_summary(self, summary: Dict[str, Any]):
        """Save training summary"""
        os.makedirs(self.config.output_dir, exist_ok=True)
        summary_path = os.path.join(self.config.output_dir, "training_summary.json")
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ðŸ’¾ Training summary saved to {summary_path}")